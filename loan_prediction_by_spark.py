# -*- coding: utf-8 -*-
"""LOAN PREDICTION BY SPARK

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EHeyGvieBOLkvxh8Kk2OWlRUIwXat9E5
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum,col
spark=SparkSession.builder.appName('loan approval by spark').getOrCreate()
df = spark.read.csv("/content/loan_approval_dataset.csv", header=True, inferSchema=True)
df.show(5)

df.printSchema()

df.describe().show()

df.select([sum(col(c).isNull().cast("int")).alias(c) for c in df.columns]).show()

df=df.drop('loan_id')

df = df.toDF(*[c.strip() for c in df.columns])

from pyspark.sql.functions import col, when,avg

df = df.withColumn("residential_assets_value", col("residential_assets_value").cast("double"))
df = df.withColumn("commercial_assets_value", col("commercial_assets_value").cast("double"))
df = df.withColumn("luxury_assets_value", col("luxury_assets_value").cast("double"))
df = df.withColumn("bank_asset_value", col("bank_asset_value").cast("double"))


df = df.withColumn("total_assets",
    col("residential_assets_value") +
    col("commercial_assets_value") +
    col("luxury_assets_value") +
    col("bank_asset_value"))

df = df.withColumn("debt_to_income", col("loan_amount") / col("income_annum"))
df = df.withColumn("cibil_flag", when(col("cibil_score") < 650, 1).otherwise(0))
df = df.withColumn("is_self_employed", when(col("self_employed") == "Yes", 1).otherwise(0))

df = df.withColumn("is_graduate", when(col("education") == "Graduate", 1).otherwise(0))



avg_income_by_education = df.groupBy("education").agg(
    avg("income_annum").alias("avg_income_by_education")
)
avg_loan_by_employment = df.groupBy("self_employed").agg(
    avg("loan_amount").alias("avg_loan_by_employment")
)

from pyspark.ml.feature import StringIndexer

indexer = StringIndexer(inputCol="education", outputCol="education_index")
for col_name in ["education", "self_employed", "loan_status"]:
    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + "_index")
    df = indexer.fit(df).transform(df)
df=df.drop("education", "self_employed", "loan_status")
df.show()

from pyspark.ml import Pipeline
from pyspark.ml.feature import Imputer, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

numeric_features = [
    'no_of_dependents', 'income_annum', 'loan_amount', 'loan_term', 'cibil_score',
    'residential_assets_value', 'commercial_assets_value', 'luxury_assets_value',
    'bank_asset_value', 'total_assets', 'debt_to_income', 'cibil_flag',
    'is_self_employed', 'is_graduate', 'education_index', 'self_employed_index'
]
imputer = Imputer(inputCols=numeric_features, outputCols=[c + "_imputed" for c in numeric_features])

assembler = VectorAssembler(
    inputCols=[c + "_imputed" for c in numeric_features],
    outputCol="features"
)

lr = LogisticRegression(featuresCol="features", labelCol="loan_status_index")

pipeline = Pipeline(stages=[imputer, assembler, lr])

paramGrid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.01, 0.1]) \
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \
    .build()

evaluator = BinaryClassificationEvaluator(
    labelCol="loan_status_index", metricName="areaUnderROC"
)

crossval = CrossValidator(
    estimator=pipeline,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    numFolds=3,
    parallelism=2
)

train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)

cvModel = crossval.fit(train_df)

predictions = cvModel.transform(test_df)
auc = evaluator.evaluate(predictions)
print(f"Test AUC: {auc:.4f}")

cv_model = crossval.fit(train_df)
best_model = cv_model.bestModel

print("Cross-validated metrics (AUC or accuracy):")
print(cv_model.avgMetrics)

import numpy as np

cv_auc_scores = [
    np.float64(0.967983617988245),
    np.float64(0.9683132130984485),
    np.float64(0.9681473797673595),
    np.float64(0.967527479166635),
    np.float64(0.9602105168077105),
    np.float64(0.9579205949146566)
]

mean_auc = np.mean(cv_auc_scores)
print(f"Mean Cross-Validated AUC: {mean_auc:.4f}")

from pyspark.ml.evaluation import BinaryClassificationEvaluator
import matplotlib.pyplot as plt
import pandas as pd

predictions = cv_model.transform(train_df)

preds_df = predictions.select("probability", "loan_status_index", "prediction") \
                      .toPandas()

preds_df['prob_1'] = preds_df['probability'].apply(lambda x: float(x[1]))

from sklearn.metrics import roc_curve, auc

fpr, tpr, thresholds = roc_curve(preds_df['loan_status_index'], preds_df['prob_1'])
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(preds_df['loan_status_index'], preds_df['prediction'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

result_df = predictions.select("loan_status_index", "prediction")

result_df.coalesce(1).write.mode("overwrite").csv("predictions_output", header=True)

import shutil
import glob
import os

# 1. Save your DataFrame as a CSV folder with coalesce(1)
result_df.coalesce(1).write.mode("overwrite").csv("predictions_output", header=True)

# 2. Find the CSV file inside the folder
csv_file = glob.glob("predictions_output/*.csv")[0]

# 3. Move and rename the CSV file to your preferred name
shutil.move(csv_file, "predictions.csv")

# 4. Delete the now-empty folder
shutil.rmtree("predictions_output")

print("CSV file saved as 'predictions.csv'")

cv_scores = [
    0.967983617988245,
    0.9683132130984485,
    0.9681473797673595,
    0.967527479166635,
    0.9602105168077105,
    0.9579205949146566
]

mean_cv_auc = 0.9650
test_auc = 0.9720

with open("model_summary.txt", "w") as f:
    f.write("Model Evaluation Summary\n")
    f.write("========================\n\n")
    f.write(f"Mean Cross-Validated AUC: {mean_cv_auc:.4f}\n")
    f.write(f"Test AUC: {test_auc:.4f}\n\n")
    f.write("Cross-Validation AUC Scores:\n")
    for i, score in enumerate(cv_scores, 1):
        f.write(f"- Fold {i}: {score:.4f}\n")
    f.write("\nComments:\n")
    f.write("- The model shows strong and consistent performance across folds.\n")
    f.write("- Slight drop in last two folds suggests possible data variability.\n")
    f.write("- Next steps: Experiment with hyperparameter tuning and feature engineering for further improvement.\n")